import os
import pandas as pd
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
import time
import re
import logging
from sentence_transformers import SentenceTransformer
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from pymongo import MongoClient
from pymongo.errors import OperationFailure
from backend.nettoyage import nettoyer_texte
from traitement_ai import extract_ticket_info

# Configuration de la journalisation
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s: %(message)s')
logger = logging.getLogger(__name__)

CONFIG = {
    "MONGODB": {
        "uri": "mongodb://localhost:27017/",
        "db_name": "jira",
        "collections": {
            "tickets": "tickets",
            "resultats": "tickets_similarite_tfidf",
            "vecteurs": "tickets_vecteurs" 
        }
    },
    "PATHS": {
        "excel_input": "C:/Users/gouja/Downloads/Ticket117.xlsx"
    },
    "SEARCH_PARAMS": {
        "max_tickets": None,
        "use_cached_vectors": True,
        "similarity_threshold": 0.00,
        "top_n_results": 5,
        "batch_size": 100,  # Pour le traitement par lots des embeddings
        "use_parallel_processing": True,  # Nouveau param√®tre pour le traitement parall√®le
        "num_workers": 4  # Nombre de workers pour le traitement parall√®le
    }
}

class RechercheTicketsEmbeddingsOptimized:
    def __init__(self, config=CONFIG):
        """
        Initialise la connexion √† MongoDB et pr√©pare le mod√®le d'embeddings.
        """
        try:
            # Stocker les param√®tres de configuration
            self.config = config
            mongodb_config = config["MONGODB"]
            search_params = config["SEARCH_PARAMS"]
            self.fichier_excel = config["PATHS"]["excel_input"]
            
            # Connexion √† MongoDB
            self.client = MongoClient(mongodb_config["uri"])
            self.db = self.client[mongodb_config["db_name"]]
            self.tickets_collection = self.db[mongodb_config["collections"]["tickets"]]
            self.resultats_collection = self.db[mongodb_config["collections"]["resultats"]]
            self.vecteurs_collection = self.db[mongodb_config["collections"]["vecteurs"]]
            
            # Initialiser le mod√®le d'embeddings
            self.model = SentenceTransformer("all-MiniLM-L6-v2")
            
            # Initialiser le stemmer et les stop words
            self.stemmer = PorterStemmer()
            self.stop_words = set(stopwords.words('english') + stopwords.words('french'))
            
            # Configurer les param√®tres de recherche
            self.max_tickets = search_params.get("max_tickets")
            self.use_cached_vectors = search_params.get("use_cached_vectors", True)
            self.similarity_threshold = search_params.get("similarity_threshold", 0.25)
            self.top_n_results = search_params.get("top_n_results", 5)
            self.batch_size = search_params.get("batch_size", 100)
            self.use_parallel = search_params.get("use_parallel_processing", True)
            self.num_workers = search_params.get("num_workers", 4)
            
            # S'assurer que la collection des vecteurs est correctement index√©e
            self._create_vector_index()
            
            logger.info(f"‚úÖ Connexion r√©ussie √† MongoDB, base '{mongodb_config['db_name']}'")
            count = self.tickets_collection.count_documents({})
            logger.info(f"üìä Nombre de tickets r√©solus dans la base: {count}")
        except Exception as e:
            logger.error(f"‚ùå Erreur d'initialisation: {e}")
            print(f"‚ùå Erreur d'initialisation: {e}")
            raise

    def _create_vector_index(self):
        """
        Cr√©e un index sur le champ embedding_vector pour acc√©l√©rer les recherches de similarit√©
        """
        try:
            # V√©rifier si l'index existe d√©j√†
            existing_indexes = self.vecteurs_collection.list_indexes()
            has_vector_index = any('embedding_vector' in idx.get('key', {}) for idx in existing_indexes)
            
            if not has_vector_index:
                logger.info("üîß Cr√©ation d'un index pour les vecteurs d'embeddings...")
                # Cr√©ation d'un index compos√© pour am√©liorer les performances de recherche
                self.vecteurs_collection.create_index([("embedding_vector", 1), ("ticket_id", 1)])
                logger.info("‚úÖ Index cr√©√© avec succ√®s")
            else:
                logger.info("‚úÖ Index d'embeddings d√©j√† existant")
                
        except OperationFailure as e:
            logger.warning(f"‚ö†Ô∏è Impossible de cr√©er l'index: {e}")
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de la cr√©ation de l'index: {e}")

    def preprocess_text(self, text):
        """
        Pr√©traite le texte pour la vectorisation
        """
        if not text or not isinstance(text, str):
            return ""
            
        try:
            # Convertir en minuscules
            text = text.lower()
            
            # Supprimer les caract√®res sp√©ciaux et la ponctuation
            text = re.sub(r'[^\w\s]', '', text)
            
            # Tokeniser le texte
            tokens = text.split()
            
            # Supprimer les stop words et appliquer stemming en une seule passe
            processed_tokens = [self.stemmer.stem(token) for token in tokens if token not in self.stop_words]
            
            # Rejoindre les tokens
            return ' '.join(processed_tokens)
        
        except Exception as e:
            logger.error(f"‚ùå Erreur de pr√©traitement du texte: {e}")
            return text if isinstance(text, str) else ""

    def formater_requete(self, ticket_text):
        """
        Formate la requ√™te de ticket avec extraction d'informations et pr√©traitement
        """
        try:
            # Nettoyer le texte
            cleaned_text = nettoyer_texte(ticket_text)

            # Pr√©traiter le texte pour la recherche
            processed_text = self.preprocess_text(cleaned_text)
            
            # Extraire les informations avec l'IA (partie √† conserver)
            extracted_info = extract_ticket_info(processed_text)
            
            if extracted_info:
                lines = extracted_info.split("\n")
                problem, keywords = "Inconnu", ""
                for i, line in enumerate(lines):
                    line = line.strip()
                    if line.lower().startswith("### probl√©matique"):
                        problem = lines[i+1].strip() if i+1 < len(lines) else "Inconnu"
                    elif line.lower().startswith("### mots-cl√©s techniques"):
                        keywords = lines[i+1].strip() if i+1 < len(lines) else ""
                
                return {
                    "problem": problem, 
                    "keywords": keywords, 
                    "original_text": cleaned_text,
                    "processed_text": processed_text
                }
            
            # Version simplifi√©e sans extraction r√©ussie
            return {
                "problem": cleaned_text,
                "original_text": cleaned_text,
                "processed_text": processed_text
            }
        except Exception as e:
            logger.error(f"‚ùå Erreur formatage requ√™te: {e}")
            # Fallback si tout √©choue
            return {
                "problem": ticket_text, 
                "original_text": ticket_text,
                "processed_text": self.preprocess_text(ticket_text) if isinstance(ticket_text, str) else ""
            }

    def verifier_vecteurs_cache(self):
        """
        V√©rifie si des embeddings sont d√©j√† stock√©s dans MongoDB
        """
        count_vecteurs = self.vecteurs_collection.count_documents({"embedding_vector": {"$exists": True}})
        count_tickets = self.tickets_collection.count_documents({})
        
        if count_vecteurs > 0:
            logger.info(f"üìä {count_vecteurs}/{count_tickets} embeddings trouv√©s en cache")
            if count_vecteurs < count_tickets:
                logger.warning(f"‚ö†Ô∏è {count_tickets - count_vecteurs} tickets n'ont pas d'embeddings")
            return count_vecteurs > 0
        return False

    def generer_embeddings_manquants(self):
        """
        G√©n√®re les embeddings pour les tickets qui n'en ont pas encore
        """
        try:
            # R√©cup√©rer les IDs des tickets qui ont d√©j√† des embeddings
            vecteurs_ids = set(str(vec["ticket_id"]) for vec in self.vecteurs_collection.find({}, {"ticket_id": 1}))
            
            # Pr√©parer le pipeline pour les tickets manquants
            pipeline = [
                {"$match": {"_id": {"$nin": list(vecteurs_ids)}}},
            ]
            
            if self.max_tickets:
                pipeline.append({"$limit": self.max_tickets})
                
            # Ex√©cuter l'agr√©gation pour obtenir les tickets manquants
            tickets_sans_vecteurs = list(self.tickets_collection.aggregate(pipeline))
            
            if not tickets_sans_vecteurs:
                logger.info("‚úÖ Tous les tickets ont des embeddings")
                return True
                
            logger.info(f"üîç G√©n√©ration d'embeddings pour {len(tickets_sans_vecteurs)} tickets...")
            
            # Traitement parall√®le si activ√©
            if self.use_parallel and len(tickets_sans_vecteurs) > 10:
                from concurrent.futures import ThreadPoolExecutor
                
                def process_batch(batch):
                    embeddings_batch = []
                    for ticket in batch:
                        problem_text = ticket.get('problem', '')
                        keywords = ticket.get('keywords', '')
                        combined_text = f"{problem_text} {keywords}"
                        embedding_vector = self.model.encode(combined_text).tolist()
                        
                        embeddings_batch.append({
                            'ticket_id': ticket.get('_id'),
                            'embedding_vector': embedding_vector,
                            'last_updated': time.time()
                        })
                    return embeddings_batch
                
                # Diviser en sous-lots pour le traitement parall√®le
                batches = [tickets_sans_vecteurs[i:i+self.batch_size] 
                          for i in range(0, len(tickets_sans_vecteurs), self.batch_size)]
                
                with ThreadPoolExecutor(max_workers=self.num_workers) as executor:
                    results = list(executor.map(process_batch, batches))
                
                # Aplatir les r√©sultats et ins√©rer
                all_embeddings = [item for sublist in results for item in sublist]
                
                # Insertion en une seule op√©ration
                if all_embeddings:
                    # Utiliser insert_many avec ordered=False pour plus de performance
                    self.vecteurs_collection.insert_many(all_embeddings, ordered=False)
                
                logger.info(f"‚úÖ {len(all_embeddings)} embeddings g√©n√©r√©s et ins√©r√©s en parall√®le")
                
            else:
                # Traitement s√©quentiel traditionnel
                for i in range(0, len(tickets_sans_vecteurs), self.batch_size):
                    batch = tickets_sans_vecteurs[i:i+self.batch_size]
                    embeddings_batch = []
                    
                    for ticket in batch:
                        problem_text = ticket.get('problem', '')
                        keywords = ticket.get('keywords', '')
                        combined_text = f"{problem_text} {keywords}"
                        embedding_vector = self.model.encode(combined_text).tolist()
                        
                        embeddings_batch.append({
                            'ticket_id': ticket.get('_id'),
                            'embedding_vector': embedding_vector,
                            'last_updated': time.time()
                        })
                    
                    # Insertion par lots
                    if embeddings_batch:
                        self.vecteurs_collection.insert_many(embeddings_batch)
                    
                    logger.info(f"‚úÖ Lot {i//self.batch_size + 1}/{(len(tickets_sans_vecteurs)-1)//self.batch_size + 1} trait√©")
            
            return True
                
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de la g√©n√©ration des embeddings: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return False

    def rechercher_tickets_similaires(self, ticket_text):
        """
        Recherche optimis√©e des tickets similaires en utilisant les embeddings
        """
        try:
            start_time = time.time()
            
            # S'assurer que tous les tickets ont des embeddings
            if not self.verifier_vecteurs_cache() or not self.generer_embeddings_manquants():
                return {"status": "error", "message": "‚ùå Impossible de g√©n√©rer les embeddings"}
            
            # Formater la requ√™te
            query_metadata = self.formater_requete(ticket_text)
            problem_text = query_metadata["problem"]
            keywords = query_metadata.get("keywords", "")
            combined_text = f"{problem_text} {keywords}"
            
            logger.info(f"üîç Recherche avec la probl√©matique: {problem_text[:100]}...")
            
            # Calculer l'embedding de la requ√™te
            query_embedding = self.model.encode(combined_text).tolist()
            
            # Am√©lioration: utiliser une approche de recherche vectorielle optimis√©e
            # R√©cup√©rer tous les vecteurs en une seule requ√™te avec projection
            vecteurs = list(self.vecteurs_collection.find({}, {"ticket_id": 1, "embedding_vector": 1, "_id": 0}))
            
            # Pr√©parer les matrices pour le calcul de similarit√©
            embeddings_matrix = [vec.get('embedding_vector', []) for vec in vecteurs]
            ticket_ids = [vec.get('ticket_id', '') for vec in vecteurs]
            
            # Calculer toutes les similarit√©s en une seule op√©ration
            similarities = cosine_similarity([query_embedding], embeddings_matrix).flatten()
            
            # Cr√©er un tableau de correspondances (id, score)
            similarity_pairs = list(zip(ticket_ids, similarities))
            
            # Filtrer et trier par score
            filtered_pairs = [(tid, score) for tid, score in similarity_pairs if score >= self.similarity_threshold]
            filtered_pairs.sort(key=lambda x: x[1], reverse=True)
            
            # Limiter aux top N r√©sultats
            top_pairs = filtered_pairs[:self.top_n_results]
            
            # R√©cup√©rer les d√©tails des tickets correspondants
            found_tickets = []
            if top_pairs:
                top_ids = [pair[0] for pair in top_pairs]
                scores_by_id = {pair[0]: pair[1] for pair in top_pairs}
                
                # Obtenir les d√©tails complets des tickets en une seule requ√™te
                tickets_details = list(self.tickets_collection.find({"_id": {"$in": top_ids}}))
                
                for ticket in tickets_details:
                    found_tickets.append({
                        "ticket_id": str(ticket.get("_id", "")),
                        "problem": ticket.get("problem", "Non sp√©cifi√©"),
                        "solution": ticket.get("solution", "Non sp√©cifi√©"),
                        "keywords": ticket.get("keywords", ""),
                        "similarity_score": float(scores_by_id.get(ticket.get("_id"), 0))
                    })
                
                # R√©tablir l'ordre par score
                found_tickets.sort(key=lambda x: x["similarity_score"], reverse=True)
            
            search_time = time.time() - start_time
            logger.info(f"‚è± Temps de recherche: {search_time:.4f} secondes")
            
            if found_tickets:
                # Stocker tous les d√©tails dans un seul document MongoDB
                resultats_document = {
                    "timestamp": time.time(),
                    "nouveau_probleme": query_metadata["problem"],
                    "resultats": found_tickets,
                    "temps_recherche": search_time
                }
                self.resultats_collection.insert_one(resultats_document)
                
                logger.info(f"‚úÖ {len(found_tickets)} tickets similaires trouv√©s")
                for ticket in found_tickets[:3]:
                    logger.info(f" - ID: {ticket['ticket_id']}, Score: {ticket.get('similarity_score', 0):.3f}")
                
                return {
                    "status": "success",
                    "message": f"‚úÖ {len(found_tickets)} tickets similaires trouv√©s en {search_time:.4f} secondes.",
                    "tickets": found_tickets,
                    "temps_recherche": search_time,
                    "query": query_metadata["problem"]
                }
            else:
                return {
                    "status": "not_found",
                    "message": "‚ùå Aucun ticket suffisamment similaire trouv√©.",
                    "temps_recherche": search_time
                }
                
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de la recherche: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return {"status": "error", "message": f"‚ùå Erreur: {str(e)}"}

    def traiter_fichier_excel(self):
        """
        Traite le fichier Excel configur√© contenant des tickets √† rechercher.
        """
        try:
            if not pd.io.common.is_file_like(self.fichier_excel) and not pd.io.common.is_url(self.fichier_excel):
                if not os.path.exists(self.fichier_excel):
                    logger.error(f"‚ùå Le fichier {self.fichier_excel} n'existe pas.")
                    return []
                    
            df = pd.read_excel(self.fichier_excel)
            resultats = []
            
            logger.info(f"üìë Traitement de {len(df)} tickets depuis le fichier Excel...")
            total_search_time = 0
            
            # Traitement parall√®le si activ√© et plus de 5 tickets √† traiter
            if self.use_parallel and len(df) > 5:
                from concurrent.futures import ThreadPoolExecutor
                
                def process_ticket(row_data):
                    idx, row = row_data
                    logger.info(f"üîç Analyse du ticket #{idx+1}...")
                    # Combiner toutes les colonnes non-nulles pour cr√©er le texte du ticket
                    raw_ticket_text = " ".join(str(value) for value in row.values if pd.notna(value))
                    # Rechercher des tickets similaires avec embeddings
                    return self.rechercher_tickets_similaires(raw_ticket_text)
                
                with ThreadPoolExecutor(max_workers=self.num_workers) as executor:
                    # Convertir le DataFrame en liste d'index et lignes
                    row_list = list(df.iterrows())
                    # Ex√©cuter en parall√®le
                    resultats = list(executor.map(process_ticket, row_list))
                
            else:
                # Traitement s√©quentiel traditionnel
                for idx, row in df.iterrows():
                    logger.info(f"üîç Analyse du ticket #{idx+1}...")
                    # Combiner toutes les colonnes non-nulles pour cr√©er le texte du ticket
                    raw_ticket_text = " ".join(str(value) for value in row.values if pd.notna(value))
                    # Rechercher des tickets similaires avec embeddings
                    resultat = self.rechercher_tickets_similaires(raw_ticket_text)
                    resultats.append(resultat)
            
            # Calculer les statistiques
            total_search_time = sum(r.get("temps_recherche", 0) for r in resultats)
            avg_search_time = total_search_time / len(df) if len(df) > 0 else 0
            
            logger.info(f"üéØ Traitement termin√©. {len(resultats)} tickets analys√©s.")
            logger.info(f"‚è±Ô∏è Temps moyen de recherche par ticket: {avg_search_time:.4f} secondes")
           
            # Calculer des statistiques
            succes = sum(1 for r in resultats if r.get("status") == "success")
            taux_succes = (succes / len(resultats)) * 100 if resultats else 0
            logger.info(f"üìä Taux de succ√®s: {taux_succes:.1f}% ({succes}/{len(resultats)})")
           
            # Sauvegarder les r√©sultats dans un fichier CSV
            self.sauvegarder_resultats_csv(resultats)
           
            return resultats
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors du traitement du fichier Excel: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return []
    
    def sauvegarder_resultats_csv(self, resultats):
        """
        Sauvegarde les r√©sultats de recherche dans un fichier CSV
        """
        if not resultats:
            logger.warning("‚ö† Aucun r√©sultat √† sauvegarder.")
            return
           
        resultats_simples = []
        for r in resultats:
            if r.get("status") == "success" and "tickets" in r:
                for ticket in r["tickets"]:
                    resultats_simples.append({
                        "probleme_original": r.get("query", ""),
                        "ticket_similaire_id": ticket["ticket_id"],
                        "probleme_similaire": ticket["problem"],
                        "solution_proposee": ticket["solution"],
                        "score_similarite": ticket["similarity_score"],
                        "temps_recherche": r.get("temps_recherche", 0)
                    })
       
        if resultats_simples:
            df_resultats = pd.DataFrame(resultats_simples)
            output_file = "resultats_recherche_embeddings.csv"
            df_resultats.to_csv(output_file, index=False)
            logger.info(f"‚úÖ R√©sultats sauvegard√©s dans '{output_file}'")
        else:
            logger.warning("‚ö† Aucun r√©sultat de similarit√© √† sauvegarder.")

def main():
    try:
        recherche = RechercheTicketsEmbeddingsOptimized()
        # V√©rifier et g√©n√©rer les embeddings si n√©cessaire
        if recherche.verifier_vecteurs_cache() or recherche.generer_embeddings_manquants():
            print(f"\nüìë Base de donn√©es pr√™te pour les recherches")
        else:
            print("‚ö† V√©rifiez que la base contient des tickets avant d'effectuer des recherches.")
        
        # Return the instance so it can be used by the FastAPI server
        return recherche
            
    except Exception as e:
        print(f"‚ùå Erreur principale: {e}")
        import traceback
        print(traceback.format_exc())
        return None

if __name__ == "__main__":
    main()