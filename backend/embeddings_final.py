import os
import pandas as pd
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
import time
import re
import json
import requests
import nltk
from nltk.corpus import stopwords
from pymongo import MongoClient
from backend.nettoyage import nettoyer_texte 
from backend.traitement_ai import extract_ticket_info
import logging
from sentence_transformers import SentenceTransformer
import pickle
import base64

# Configuration de la journalisation
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s: %(message)s')
logger = logging.getLogger(__name__)

LM_STUDIO_API = "http://localhost:1234/v1/chat/completions"
CONFIG = {
    "MONGODB": {
        "uri": "mongodb://localhost:27017/",
        "db_name": "jira",
        "collections": {
            "tickets": "tickets",
            "resultats": "tickets_similarite_embeddings",  # Chang√© pour distinguer des r√©sultats TF-IDF
            "embeddings": "tickets_embeddings"  # Collection d√©di√©e aux embeddings
        }
    },
    "PATHS": {
        "excel_input": "" # Ce sera remplac√© dynamiquement
    },
    "SEARCH_PARAMS": {
        "max_tickets": None,
        "use_cached_embeddings": True,
        "similarity_threshold": 0.65,  # Seuil ajust√© pour les embeddings
        "top_n_results": 5,
        "use_ai_refinement": False,
        "embedding_model": "all-MiniLM-L6-v2"  # Mod√®le d'embedding choisi
    }
}

class RechercheTicketsEmbeddingsOptimized:
    def __init__(self, config=CONFIG):
        """
        Initialise la connexion √† MongoDB et pr√©pare le mod√®le d'embeddings.
        """
        try:
            # Stocker les param√®tres de configuration
            self.config = config
            mongodb_config = config["MONGODB"]
            search_params = config["SEARCH_PARAMS"]
            
            # D√©finir l'attribut fichier_excel
            self.fichier_excel = config["PATHS"]["excel_input"]
           
            # Connexion √† MongoDB
            self.client = MongoClient(mongodb_config["uri"])
            self.db = self.client[mongodb_config["db_name"]]
            self.tickets_collection = self.db[mongodb_config["collections"]["tickets"]]
            self.resultats_collection = self.db[mongodb_config["collections"]["resultats"]]
            self.embeddings_collection = self.db[mongodb_config["collections"]["embeddings"]]
            self.use_ai_refinement = search_params.get("use_ai_refinement", True)

            # Initialiser le mod√®le d'embeddings
            model_name = search_params.get("embedding_model", "all-MiniLM-L6-v2")
            self.embedding_model = SentenceTransformer(model_name)
            self.embedding_matrix = None
            self.tickets_db = None
           
            # Initialiser les stop words
            try:
                self.stop_words = set(stopwords.words('english') + stopwords.words('french'))
            except LookupError:
                # T√©l√©charger les stopwords si n√©cessaire
                nltk.download('stopwords')
                self.stop_words = set(stopwords.words('english') + stopwords.words('french'))
           
            # Configurer les param√®tres de recherche
            self.max_tickets = search_params.get("max_tickets")
            self.use_cached_embeddings = search_params.get("use_cached_embeddings", True)
            self.similarity_threshold = search_params.get("similarity_threshold", 0.65)
            self.top_n_results = search_params.get("top_n_results", 5)
           
            print(f"‚úÖ Connexion r√©ussie √† MongoDB, base '{mongodb_config['db_name']}'")
            count = self.tickets_collection.count_documents({})
            print(f"üìä Nombre de tickets r√©solus dans la base: {count}")
            print(f"üß† Mod√®le d'embeddings initialis√©: {model_name}")

            logger.info(f"‚úÖ Connexion r√©ussie √† MongoDB, base '{mongodb_config['db_name']}'")
            logger.info(f"üìä Nombre de tickets r√©solus dans la base: {count}")
            logger.info(f"üß† Mod√®le d'embeddings initialis√©: {model_name}")
           
        except Exception as e:
            logger.error(f"‚ùå Erreur d'initialisation: {e}")
            print(f"‚ùå Erreur d'initialisation: {e}")
            raise

    def formater_requete(self, ticket_text):
        """
        Formate la requ√™te de ticket avec extraction d'informations
        """
        try:
            # Nettoyer le texte
            cleaned_text = nettoyer_texte(ticket_text)
            
            # Extraire les informations
            extracted_info = extract_ticket_info(cleaned_text)
            
            if extracted_info:
                lines = extracted_info.split("\n")
                problem, keywords = "Inconnu", ""
                for i, line in enumerate(lines):
                    line = line.strip()
                    if line.lower().startswith("### probl√©matique"):
                        problem = lines[i+1].strip() if i+1 < len(lines) else "Inconnu"
                    elif line.lower().startswith("### mots-cl√©s techniques"):
                        keywords = lines[i+1].strip() if i+1 < len(lines) else ""
                
                return {
                    "problem": problem, 
                    "keywords": keywords, 
                    "original_text": cleaned_text
                }
            else:
                return {
                    "problem": cleaned_text, 
                    "original_text": cleaned_text
                }
        except Exception as e:
            logger.error(f"‚ùå Erreur formatage requ√™te: {e}")
            # Fallback si tout √©choue
            return {
                "problem": ticket_text, 
                "original_text": ticket_text
            }

    def refine_similarity_with_ai(self, base_ticket, similar_tickets):
        """
        Utilise l'API LM Studio pour affiner la similarit√© des tickets avec analyse IA
        """
        try:
            if not similar_tickets:
                return []

            # Formater les tickets similaires pour le prompt
            tickets_text = "\n\n".join([
                f"Ticket ID: {ticket['ticket_id']}\n"
                f"Probl√®me: {ticket['problem']}\n"
                f"Solution: {ticket['solution']}\n"
                f"Score de similarit√© initial: {ticket['similarity_score']:.2f}%"
                for ticket in similar_tickets
            ])

            prompt = f"""
            T√¢che: Analyser la similarit√© s√©mantique entre un ticket de base et des tickets similaires.

            Instructions:
            1. Comparez chaque ticket avec le ticket de base
            2. √âvaluez la similarit√© s√©mantique (0-100%)
            3. Identifiez les aspects techniques correspondants
            4. Fournissez une br√®ve explication de pertinence

            Ticket de base:
            {base_ticket}

            Tickets similaires:
            {tickets_text}

            FORMAT DE R√âPONSE STRICTEMENT JSON:
            [
                {{
                    "ticket_id": "ID du ticket",
                    "similarite_semantique": 75,
                    "aspects_correspondants": ["aspect1", "aspect2"],
                    "explication_pertinence": "Explication concise"
                }}
            ]
            """

            payload = {
                "model": "mistral-nemo-instruct-2407",
                "messages": [{"role": "user", "content": prompt}],
                "temperature": 0.3,
                "max_tokens": 1000
            }

            logger.info("üì§ Envoi de la requ√™te √† l'API IA...")
            response = requests.post(LM_STUDIO_API, json=payload, timeout=180)
            response.raise_for_status()
            
            result = response.json()
            full_response = result["choices"][0]["message"]["content"]
            
            logger.info("üîç R√©ponse compl√®te de l'IA re√ßue")

            # Tentatives multiples de parsing JSON
            try:
                # M√©thode 1 : Parsing JSON direct
                refined_tickets = json.loads(full_response)
                logger.info("‚úÖ Parsing JSON direct r√©ussi")
                return refined_tickets
            except json.JSONDecodeError:
                # M√©thode 2 : Extraction JSON entre ```json et ```
                json_match = re.search(r'```json(.*?)```', full_response, re.DOTALL)
                if json_match:
                    try:
                        refined_tickets = json.loads(json_match.group(1))
                        logger.info("‚úÖ Parsing JSON entre ``` r√©ussi")
                        return refined_tickets
                    except json.JSONDecodeError:
                        logger.warning("‚ùå Parsing JSON entre ``` √©chou√©")

                # M√©thode 3 : Extraction JSON personnalis√©e
                logger.warning("‚ö†Ô∏è Tentative d'extraction JSON personnalis√©e")
                return self._parse_ai_response(full_response)

        except Exception as e:
            logger.error(f"‚ùå Erreur de raffinement par IA: {e}")
            return []

    def _parse_ai_response(self, ai_response):
        """
        M√©thode de repli pour parser la r√©ponse de l'IA
        """
        try:
            # Ajout de regex plus robuste
            refined_tickets = []
            pattern = re.compile(
                r'"ticket_id"\s*:\s*"?(\S+)"?.*'
                r'"similarite_semantique"\s*:\s*(\d+).*'
                r'"aspects_correspondants"\s*:\s*\[(.*?)\].*'
                r'"explication_pertinence"\s*:\s*"(.*?)"',
                re.DOTALL | re.IGNORECASE
            )

            matches = list(pattern.finditer(ai_response))
            
            for match in matches:
                ticket_id = match.group(1).strip('"')
                similarite = int(match.group(2))
                aspects = [aspect.strip().strip('"') for aspect in match.group(3).split(',') if aspect.strip()]
                explication = match.group(4).strip()

                refined_tickets.append({
                    "ticket_id": ticket_id,
                    "similarite_semantique": similarite,
                    "aspects_correspondants": aspects,
                    "explication_pertinence": explication
                })

            logger.info(f"‚úÖ Extraction manuelle : {len(refined_tickets)} tickets raffin√©s")
            return refined_tickets

        except Exception as e:
            logger.error(f"‚ùå √âchec de l'extraction manuelle : {e}")
            return []

    def encoder_embedding_pour_mongodb(self, embedding_array):
        """
        Encode un embedding numpy pour le stockage dans MongoDB
        """
        # Convertir l'array numpy en bytes via pickle
        pickled = pickle.dumps(embedding_array)
        # Encoder en base64 pour le stockage sous forme de cha√Æne
        return base64.b64encode(pickled).decode('utf-8')
        
    def decoder_embedding_depuis_mongodb(self, encoded_embedding):
        """
        D√©code un embedding stock√© dans MongoDB en array numpy
        """
        # D√©coder depuis base64
        pickled = base64.b64decode(encoded_embedding)
        # Reconstruire l'array numpy
        return pickle.loads(pickled)

    def verifier_embeddings_cache(self):
        """
        V√©rifie si des embeddings sont d√©j√† stock√©s dans MongoDB
        """
        count = self.embeddings_collection.count_documents({})
        tickets_count = self.tickets_collection.count_documents({})
       
        # V√©rifie si le nombre d'embeddings correspond au nombre de tickets
        if count > 0 and count >= tickets_count:
            print(f"‚úÖ Utilisation de {count} embeddings en cache")
            return True
        print(f"‚ö†Ô∏è Cache d'embeddings incomplet ({count}/{tickets_count})")
        return False

    def charger_tickets_depuis_mongo(self):
        """
        Charge les tickets depuis MongoDB et construit la matrice d'embeddings
        """
        try:
            start_time = time.time()
            print("üìù Chargement des tickets depuis MongoDB...")
            
            # Charger tous les tickets
            query = {}
            if self.max_tickets:
                cursor = self.tickets_collection.find(query).limit(self.max_tickets)
            else:
                cursor = self.tickets_collection.find(query)
           
            self.tickets_db = list(cursor)
           
            if not self.tickets_db:
                print("‚ö† Aucun ticket trouv√© dans la base")
                return False
               
            print(f"‚úÖ {len(self.tickets_db)} tickets charg√©s")
            
            # V√©rifier si on peut utiliser des embeddings en cache
            if self.use_cached_embeddings and self.verifier_embeddings_cache():
                # Cr√©er un dictionnaire pour mapping ticket_id -> embedding
                cached_embeddings = {}
                cursor = self.embeddings_collection.find({})
                
                for doc in cursor:
                    if 'ticket_id' in doc and 'embedding_data' in doc:
                        cached_embeddings[str(doc['ticket_id'])] = self.decoder_embedding_depuis_mongodb(doc['embedding_data'])
                
                # V√©rifier que nous avons des embeddings pour tous les tickets
                embeddings_list = []
                missing_tickets = []
                
                for ticket in self.tickets_db:
                    ticket_id = str(ticket.get('_id'))
                    if ticket_id in cached_embeddings:
                        embeddings_list.append(cached_embeddings[ticket_id])
                    else:
                        missing_tickets.append(ticket)
                
                # S'il manque des embeddings, les g√©n√©rer
                if missing_tickets:
                    print(f"‚ö†Ô∏è G√©n√©ration d'embeddings pour {len(missing_tickets)} tickets manquants...")
                    missing_embeddings = self._generate_embeddings([
                        f"{ticket.get('problem', '')} {ticket.get('keywords', '')}" 
                        for ticket in missing_tickets
                    ])
                    
                    # Ajouter les nouveaux embeddings √† la liste et les sauvegarder
                    for i, ticket in enumerate(missing_tickets):
                        ticket_id = str(ticket.get('_id'))
                        embeddings_list.append(missing_embeddings[i])
                        
                        # Sauvegarder dans MongoDB
                        encoded_emb = self.encoder_embedding_pour_mongodb(missing_embeddings[i])
                        self.embeddings_collection.update_one(
                            {'ticket_id': ticket.get('_id')},
                            {'$set': {
                                'ticket_id': ticket.get('_id'),
                                'embedding_data': encoded_emb,
                                'embedding_size': len(missing_embeddings[i]),
                                'last_updated': time.time()
                            }},
                            upsert=True
                        )
                
                # Assembler la matrice finale
                self.embedding_matrix = np.vstack(embeddings_list)
                
                load_time = time.time() - start_time
                print(f"‚è±Ô∏è Temps de chargement des embeddings: {load_time:.2f} secondes")
                return True
                
            else:
                # G√©n√©rer tous les embeddings √† nouveau
                print("üß† G√©n√©ration de nouveaux embeddings pour tous les tickets...")
                texts_to_embed = [
                    f"{ticket.get('problem', '')} {ticket.get('keywords', '')}" 
                    for ticket in self.tickets_db
                ]
                
                self.embedding_matrix = self._generate_embeddings(texts_to_embed)
                
                # Sauvegarder les embeddings si n√©cessaire
                if self.use_cached_embeddings:
                    print("üíæ Sauvegarde des embeddings dans MongoDB...")
                    
                    # Vider la collection d'embeddings existante
                    self.embeddings_collection.delete_many({})
                    
                    # Ins√©rer les nouveaux embeddings
                    for i, ticket in enumerate(self.tickets_db):
                        encoded_emb = self.encoder_embedding_pour_mongodb(self.embedding_matrix[i])
                        self.embeddings_collection.insert_one({
                            'ticket_id': ticket.get('_id'),
                            'embedding_data': encoded_emb,
                            'embedding_size': len(self.embedding_matrix[i]),
                            'last_updated': time.time()
                        })
                
                gen_time = time.time() - start_time
                print(f"‚è±Ô∏è Temps de g√©n√©ration des embeddings: {gen_time:.2f} secondes")
                return True
                
        except Exception as e:
            print(f"‚ùå Erreur lors du chargement des tickets: {e}")
            logger.error(f"‚ùå Erreur lors du chargement des tickets: {e}")
            return False

    def _generate_embeddings(self, texts):
        """
        G√©n√®re des embeddings pour une liste de textes
        """
        # Traiter par lots pour √©viter les probl√®mes de m√©moire
        batch_size = 32
        all_embeddings = []
        
        for i in range(0, len(texts), batch_size):
            batch_texts = texts[i:i+batch_size]
            batch_embeddings = self.embedding_model.encode(batch_texts)
            all_embeddings.append(batch_embeddings)
            print(f"  - Traitement des embeddings: {min(i+batch_size, len(texts))}/{len(texts)}")
        
        # Concat√©ner tous les embeddings
        if len(all_embeddings) == 1:
            return all_embeddings[0]
        else:
            return np.vstack(all_embeddings)

    def rechercher_tickets_similaires(self, ticket_text):
        """
        Recherche les tickets similaires en utilisant les embeddings
        """
        try:
            start_time = time.time()
       
            if self.embedding_matrix is None:
                success = self.charger_tickets_depuis_mongo()
                if not success:
                    return {"status": "error", "message": "‚ùå Impossible de charger les tickets"}
       
            query_metadata = self.formater_requete(ticket_text)
            query_text = f"{query_metadata['problem']} {query_metadata.get('keywords', '')}"
       
            logger.info(f"üîç Recherche avec la probl√©matique: {query_metadata['problem'][:100]}...")
       
            # G√©n√©rer l'embedding pour la requ√™te
            query_embedding = self.embedding_model.encode([query_text])[0].reshape(1, -1)
            
            # Calculer les similarit√©s
            similarities = cosine_similarity(query_embedding, self.embedding_matrix).flatten()
       
            found_tickets = []
            for i, similarity in enumerate(similarities):
                if similarity >= self.similarity_threshold:
                    ticket = self.tickets_db[i]
                    found_tickets.append({
                        "ticket_id": str(ticket.get("_id", ticket.get("ID", ""))),
                        "problem": ticket.get("problem", "Non sp√©cifi√©"),
                        "solution": ticket.get("solution", "Non sp√©cifi√©"),
                        "keywords": ticket.get("keywords", ""),
                        "similarity_score": float(similarity * 100)  # Convertir en pourcentage
                    })
       
            found_tickets.sort(key=lambda x: x["similarity_score"], reverse=True)
            found_tickets = found_tickets[:self.top_n_results]
           
            # Raffinement par IA si activ√©
            raffinement_details = {}
            if self.use_ai_refinement and found_tickets:
                logger.info("ü§ñ Raffinement des r√©sultats par IA...")
                refined_tickets = self.refine_similarity_with_ai(query_metadata["original_text"], found_tickets)
                
                # Pr√©parer les informations de raffinement pour MongoDB
                if refined_tickets:
                    for ticket in found_tickets:
                        refined_info = next((rt for rt in refined_tickets if rt["ticket_id"] == ticket["ticket_id"]), None)
                        if refined_info:
                            raffinement_details[ticket["ticket_id"]] = {
                                "semantic_similarity": refined_info.get("similarite_semantique"),
                                "corresponding_aspects": refined_info.get("aspects_correspondants"),
                                "relevance_explanation": refined_info.get("explication_pertinence")
                            }
           
            search_time = time.time() - start_time
            logger.info(f"‚è±Ô∏è Temps de recherche: {search_time:.4f} secondes")
       
            if found_tickets:
                # Stocker tous les d√©tails dans un seul document MongoDB
                resultats_document = {
                    "timestamp": time.time(),
                    "nouveau_probleme": query_metadata["problem"],
                    "resultats": found_tickets,
                    "raffinement_ia": raffinement_details if self.use_ai_refinement else None,
                    "temps_recherche": search_time
                }
                
                self.resultats_collection.insert_one(resultats_document)
           
                logger.info(f"‚úÖ {len(found_tickets)} tickets similaires trouv√©s")
                for ticket in found_tickets[:3]:
                    logger.info(f"  - ID: {ticket['ticket_id']}, Score: {ticket.get('similarity_score', 'N/A'):.2f}%")
               
                return {
                    "status": "success",
                    "message": f"‚úÖ {len(found_tickets)} tickets similaires trouv√©s en {search_time:.4f} secondes.",
                    "tickets": found_tickets,
                    "temps_recherche": search_time,
                    "query": query_metadata["problem"]
                }
           
            else:
                return {
                    "status": "not_found",
                    "message": "‚ùå Aucun ticket suffisamment similaire trouv√©.",
                    "temps_recherche": search_time
                }
       
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de la recherche: {e}")
            return {"status": "error", "message": f"‚ùå Erreur: {str(e)}"}

    def traiter_fichier_excel(self):
        """
        Traite le fichier Excel configur√© contenant des tickets √† rechercher.
        Version utilisant les embeddings.
        """
        try:
            if not os.path.exists(self.fichier_excel):
                print(f"‚ùå Le fichier {self.fichier_excel} n'existe pas.")
                return []
               
            df = pd.read_excel(self.fichier_excel)
            resultats = []
           
            # S'assurer que les tickets sont charg√©s et les embeddings g√©n√©r√©s
            if self.embedding_matrix is None:
                success = self.charger_tickets_depuis_mongo()
                if not success:
                    return []
           
            print(f"üìë Traitement de {len(df)} tickets depuis le fichier Excel...")
           
            total_search_time = 0
            for idx, row in df.iterrows():
                print(f"\nüîç Analyse du ticket #{idx+1}...")
               
                # Combiner toutes les colonnes non-nulles pour cr√©er le texte du ticket
                raw_ticket_text = " ".join(str(value) for value in row.values if pd.notna(value))
               
                # Rechercher des tickets similaires avec embeddings
                resultat = self.rechercher_tickets_similaires(raw_ticket_text)
                resultats.append(resultat)
               
                if "temps_recherche" in resultat:
                    total_search_time += resultat["temps_recherche"]
           
            avg_search_time = total_search_time / len(df) if len(df) > 0 else 0
            print(f"\nüéØ Traitement termin√©. {len(resultats)} tickets analys√©s.")
            print(f"‚è±Ô∏è Temps moyen de recherche par ticket: {avg_search_time:.4f} secondes")
           
            # Calculer des statistiques
            succes = sum(1 for r in resultats if r.get("status") == "success")
            taux_succes = (succes / len(resultats)) * 100 if resultats else 0
            print(f"üìä Taux de succ√®s: {taux_succes:.1f}% ({succes}/{len(resultats)})")
           
            # Sauvegarder les r√©sultats dans un fichier CSV
            self.sauvegarder_resultats_csv(resultats)
           
            return resultats
        except Exception as e:
            print(f"‚ùå Erreur lors du traitement du fichier Excel: {e}")
            return []
   
    def sauvegarder_resultats_csv(self, resultats):
        """
        Sauvegarde les r√©sultats de recherche dans un fichier CSV
        """
        if not resultats:
            print("‚ö† Aucun r√©sultat √† sauvegarder.")
            return
           
        resultats_simples = []
        for r in resultats:
            if r.get("status") == "success" and "tickets" in r:
                for ticket in r["tickets"]:
                    resultats_simples.append({
                        "probleme_original": r.get("query", ""),
                        "ticket_similaire_id": ticket["ticket_id"],
                        "probleme_similaire": ticket["problem"],
                        "solution_proposee": ticket["solution"],
                        "score_similarite": ticket["similarity_score"],
                        "temps_recherche": r.get("temps_recherche", 0)
                    })
       
        if resultats_simples:
            df_resultats = pd.DataFrame(resultats_simples)
            output_file = "resultats_recherche_embeddings.csv"
            df_resultats.to_csv(output_file, index=False)
            print(f"‚úÖ R√©sultats sauvegard√©s dans '{output_file}'")
        else:
            print("‚ö† Aucun r√©sultat de similarit√© √† sauvegarder.")

def main():
    try:
        recherche = RechercheTicketsEmbeddingsOptimized()
        # V√©rifier et g√©n√©rer les embeddings si n√©cessaire
        if recherche.verifier_vecteurs_cache() and recherche.generer_embeddings_manquants():
            print(f"\nüìë Configuration pr√™te pour traiter des fichiers")
        else:
            print("‚ö† V√©rifiez que la base contient des tickets avant d'effectuer des recherches.")
        return recherche
    except Exception as e:
        print(f"‚ùå Erreur principale: {e}")
        import traceback
        print(traceback.format_exc())
        return None
if __name__ == "__main__":
    main()