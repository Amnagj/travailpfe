import os
import pandas as pd
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
import time
import re
import json
import requests
import logging
from sentence_transformers import SentenceTransformer
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from pymongo import MongoClient
from pymongo.errors import OperationFailure
from backend.nettoyage import nettoyer_texte 
from backend.traitement_ai import extract_ticket_info

# Configuration de la journalisation
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s: %(message)s')
logger = logging.getLogger(__name__)

LM_STUDIO_API = "http://localhost:1234/v1/chat/completions"
CONFIG = {
    "MONGODB": {
        "uri": "mongodb://localhost:27017/",
        "db_name": "jira",
        "collections": {
            "tickets": "tickets",
            "resultats": "tickets_similarite_tfidf",
            "vecteurs": "tickets_vecteurs" 
        }
    },
    #"PATHS": {
      #  "excel_input": "C:/Users/gouja/Downloads/Ticket117.xlsx"
   # },
    "SEARCH_PARAMS": {
        "max_tickets": None,
        "use_cached_vectors": True,
        "similarity_threshold": 0.00,
        "top_n_results": 5,
        "use_ai_refinement": True,  # D√©sactiv√© par d√©faut pour optimiser les performances
        "batch_size": 100  # Pour le traitement par lots des embeddings
    }
}


# Utilise-les explicitement pour initialiser le moteur de recherche
class RechercheTicketsEmbeddingsOptimized:
    def __init__(self, config=CONFIG):
        """
        Initialise la connexion √† MongoDB et pr√©pare le mod√®le d'embeddings.
        """
        try:
            # Stocker les param√®tres de configuration
            self.config = config
            mongodb_config = config["MONGODB"]
            search_params = config["SEARCH_PARAMS"]
            #self.fichier_excel = config["PATHS"]["excel_input"]
            
            # Connexion √† MongoDB
            self.client = MongoClient(mongodb_config["uri"])
            self.db = self.client[mongodb_config["db_name"]]
            self.tickets_collection = self.db[mongodb_config["collections"]["tickets"]]
            self.resultats_collection = self.db[mongodb_config["collections"]["resultats"]]
            self.vecteurs_collection = self.db[mongodb_config["collections"]["vecteurs"]]
            
            # Initialiser le mod√®le d'embeddings
            self.model = SentenceTransformer("all-MiniLM-L6-v2")
            
            # Initialiser le stemmer et les stop words
            self.stemmer = PorterStemmer()
            self.stop_words = set(stopwords.words('english') + stopwords.words('french'))
            
            # Configurer les param√®tres de recherche
            self.max_tickets = search_params.get("max_tickets")
            self.use_cached_vectors = search_params.get("use_cached_vectors", True)
            self.similarity_threshold = search_params.get("similarity_threshold", 0.25)
            self.top_n_results = search_params.get("top_n_results", 5)
            self.use_ai_refinement = search_params.get("use_ai_refinement", True)
            self.batch_size = search_params.get("batch_size", 100)
            
            # S'assurer que la collection des vecteurs est correctement index√©e
            self._create_vector_index()
            
            logger.info(f"‚úÖ Connexion r√©ussie √† MongoDB, base '{mongodb_config['db_name']}'")
            count = self.tickets_collection.count_documents({})
            logger.info(f"üìä Nombre de tickets r√©solus dans la base: {count}")
        except Exception as e:
            logger.error(f"‚ùå Erreur d'initialisation: {e}")
            print(f"‚ùå Erreur d'initialisation: {e}")
            raise

    def _create_vector_index(self):
        """
        Cr√©e un index sur le champ embedding_vector pour acc√©l√©rer les recherches de similarit√©
        """
        try:
            # V√©rifier si l'index existe d√©j√†
            existing_indexes = self.vecteurs_collection.list_indexes()
            has_vector_index = any('embedding_vector' in idx.get('key', {}) for idx in existing_indexes)
            
            if not has_vector_index:
                logger.info("üîß Cr√©ation d'un index pour les vecteurs d'embeddings...")
                self.vecteurs_collection.create_index([("embedding_vector", 1)])
                logger.info("‚úÖ Index cr√©√© avec succ√®s")
            else:
                logger.info("‚úÖ Index d'embeddings d√©j√† existant")
                
        except OperationFailure as e:
            logger.warning(f"‚ö†Ô∏è Impossible de cr√©er l'index: {e}")
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de la cr√©ation de l'index: {e}")

    def preprocess_text(self, text):
        """
        Pr√©traite le texte pour la vectorisation
        """
        try:
            # Convertir en minuscules
            text = text.lower()
            
            # Supprimer les caract√®res sp√©ciaux et la ponctuation
            text = re.sub(r'[^\w\s]', '', text)
            
            # Tokeniser le texte
            tokens = text.split()
            
            # Supprimer les stop words
            tokens = [token for token in tokens if token not in self.stop_words]
            
            # Stemmatisation des tokens
            tokens = [self.stemmer.stem(token) for token in tokens]
            
            # Rejoindre les tokens
            processed_text = ' '.join(tokens)
            
            return processed_text
        
        except Exception as e:
            logger.error(f"‚ùå Erreur de pr√©traitement du texte: {e}")
            return text

    def formater_requete(self, ticket_text):
        """
        Formate la requ√™te de ticket avec extraction d'informations et pr√©traitement
        """
        try:
            # Nettoyer le texte
            cleaned_text = nettoyer_texte(ticket_text)

            # Pr√©traiter le texte pour la recherche
            processed_text = self.preprocess_text(cleaned_text)
            
            # Extraire les informations si l'IA est activ√©e
            if self.use_ai_refinement:
                extracted_info = extract_ticket_info(processed_text)
                
                if extracted_info:
                    lines = extracted_info.split("\n")
                    problem, keywords = "Inconnu", ""
                    for i, line in enumerate(lines):
                        line = line.strip()
                        if line.lower().startswith("### probl√©matique"):
                            problem = lines[i+1].strip() if i+1 < len(lines) else "Inconnu"
                        elif line.lower().startswith("### mots-cl√©s techniques"):
                            keywords = lines[i+1].strip() if i+1 < len(lines) else ""
                    
                    return {
                        "problem": problem, 
                        "keywords": keywords, 
                        "original_text": cleaned_text,
                        "processed_text": processed_text
                    }
            
            # Version simplifi√©e sans IA
            return {
                "problem": cleaned_text,
                "original_text": cleaned_text,
                "processed_text": processed_text
            }
        except Exception as e:
            logger.error(f"‚ùå Erreur formatage requ√™te: {e}")
            # Fallback si tout √©choue
            return {
                "problem": ticket_text, 
                "original_text": ticket_text,
                "processed_text": self.preprocess_text(ticket_text)
            }

    def verifier_vecteurs_cache(self):
        """
        V√©rifie si des embeddings sont d√©j√† stock√©s dans MongoDB
        """
        count_vecteurs = self.vecteurs_collection.count_documents({"embedding_vector": {"$exists": True}})
        count_tickets = self.tickets_collection.count_documents({})
        
        if count_vecteurs > 0:
            logger.info(f"üìä {count_vecteurs}/{count_tickets} embeddings trouv√©s en cache")
            if count_vecteurs < count_tickets:
                logger.warning(f"‚ö†Ô∏è {count_tickets - count_vecteurs} tickets n'ont pas d'embeddings")
            return count_vecteurs > 0
        return False

    def generer_embeddings_manquants(self):
        """
        G√©n√®re les embeddings pour les tickets qui n'en ont pas encore
        """
        try:
            # R√©cup√©rer les IDs des tickets qui ont d√©j√† des embeddings
            vecteurs_ids = set(str(vec["ticket_id"]) for vec in self.vecteurs_collection.find({}, {"ticket_id": 1}))
            
            # R√©cup√©rer les tickets qui n'ont pas d'embeddings
            tickets_sans_vecteurs = []
            if self.max_tickets:
                cursor = self.tickets_collection.find({"_id": {"$nin": list(vecteurs_ids)}}).limit(self.max_tickets)
            else:
                cursor = self.tickets_collection.find({"_id": {"$nin": list(vecteurs_ids)}})
            
            tickets_sans_vecteurs = list(cursor)
            
            if not tickets_sans_vecteurs:
                logger.info("‚úÖ Tous les tickets ont des embeddings")
                return True
                
            logger.info(f"üîç G√©n√©ration d'embeddings pour {len(tickets_sans_vecteurs)} tickets...")
            
            # Traiter par lots pour √©viter une surcharge de m√©moire
            for i in range(0, len(tickets_sans_vecteurs), self.batch_size):
                batch = tickets_sans_vecteurs[i:i+self.batch_size]
                embeddings_batch = []
                
                for ticket in batch:
                    problem_text = ticket.get('problem', '')
                    keywords = ticket.get('keywords', '')
                    combined_text = f"{problem_text} {keywords}"
                    embedding_vector = self.model.encode(combined_text).tolist()
                    
                    # Pr√©parer le document √† ins√©rer
                    embeddings_batch.append({
                        'ticket_id': ticket.get('_id'),
                        'embedding_vector': embedding_vector,
                        'last_updated': time.time()
                    })
                
                # Insertion par lots
                if embeddings_batch:
                    self.vecteurs_collection.insert_many(embeddings_batch)
                
                logger.info(f"‚úÖ Lot {i//self.batch_size + 1}/{(len(tickets_sans_vecteurs)-1)//self.batch_size + 1} trait√©")
            
            return True
                
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de la g√©n√©ration des embeddings: {e}")
            return False

    def rechercher_tickets_similaires(self, ticket_text):
        """
        Recherche optimis√©e des tickets similaires en utilisant les embeddings
        """
        try:
            start_time = time.time()
            
            # S'assurer que tous les tickets ont des embeddings
            if not self.verifier_vecteurs_cache() or not self.generer_embeddings_manquants():
                return {"status": "error", "message": "‚ùå Impossible de g√©n√©rer les embeddings"}
            
            # Formater la requ√™te
            query_metadata = self.formater_requete(ticket_text)
            problem_text = query_metadata["problem"]
            keywords = query_metadata.get("keywords", "")
            combined_text = f"{problem_text} {keywords}"
            
            logger.info(f"üîç Recherche avec la probl√©matique: {problem_text[:100]}...")
            # Dans rechercher_tickets_similaires, avant de calculer l'embedding de la requ√™te
            logger.info(f"Texte de recherche apr√®s pr√©traitement: {combined_text[:300]}...")
            logger.info(f"Configuration de recherche utilis√©e: {self.similarity_threshold}, {self.top_n_results}")
            # Calculer l'embedding de la requ√™te
            query_embedding = self.model.encode(combined_text).tolist()
            
            # R√©cup√©rer les embeddings depuis MongoDB par lots pour comparer
            # Cette approche √©vite de charger tous les embeddings en m√©moire d'un coup
            found_tickets = []
            
            # Nombre total de tickets √† traiter
            total_tickets = self.vecteurs_collection.count_documents({})
            
            # Recherche par lots
            for skip in range(0, total_tickets, self.batch_size):
                vecteurs_batch = list(self.vecteurs_collection.find().skip(skip).limit(self.batch_size))
                
                if not vecteurs_batch:
                    break
                    
                # Extraire les embeddings et IDs du lot
                embeddings_batch = [vec.get('embedding_vector', []) for vec in vecteurs_batch]
                ticket_ids_batch = [vec.get('ticket_id', '') for vec in vecteurs_batch]
                
                # Calculer les similarit√©s pour ce lot
                similarities_batch = cosine_similarity([query_embedding], embeddings_batch).flatten()
                
                # Ajouter les tickets similaires au-dessus du seuil
                for i, similarity in enumerate(similarities_batch):
                    if similarity >= self.similarity_threshold:
                        ticket_id = ticket_ids_batch[i]
                        # R√©cup√©rer les d√©tails du ticket depuis la collection principale
                        ticket = self.tickets_collection.find_one({"_id": ticket_id})
                        
                        if ticket:
                            found_tickets.append({
                                "ticket_id": str(ticket.get("_id", "")),
                                "problem": ticket.get("problem", "Non sp√©cifi√©"),
                                "solution": ticket.get("solution", "Non sp√©cifi√©"),
                                "keywords": ticket.get("keywords", ""),
                                "similarity_score": float(similarity)
                            })
            
            # Trier par score de similarit√© et limiter aux top N r√©sultats
            found_tickets.sort(key=lambda x: x["similarity_score"], reverse=True)
            found_tickets = found_tickets[:self.top_n_results]
            
            # Raffinement par IA si activ√©
            raffinement_details = {}
            
            search_time = time.time() - start_time
            logger.info(f"‚è± Temps de recherche: {search_time:.4f} secondes")
            
            if found_tickets:
                # Stocker tous les d√©tails dans un seul document MongoDB
                resultats_document = {
                    "timestamp": time.time(),
                    "nouveau_probleme": query_metadata["problem"],
                    "resultats": found_tickets,
                    "raffinement_ia": raffinement_details if self.use_ai_refinement else None,
                    "temps_recherche": search_time
                }
                self.resultats_collection.insert_one(resultats_document)
                
                logger.info(f"‚úÖ {len(found_tickets)} tickets similaires trouv√©s")
                for ticket in found_tickets[:3]:
                    logger.info(f" - ID: {ticket['ticket_id']}, Score: {ticket.get('similarity_score', 0):.3f}")
                
                return {
                    "status": "success",
                    "message": f"‚úÖ {len(found_tickets)} tickets similaires trouv√©s en {search_time:.4f} secondes.",
                    "tickets": found_tickets,
                    "temps_recherche": search_time,
                    "query": query_metadata["problem"]
                }
            else:
                return {
                    "status": "not_found",
                    "message": "‚ùå Aucun ticket suffisamment similaire trouv√©.",
                    "temps_recherche": search_time
                }
                
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de la recherche: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return {"status": "error", "message": f"‚ùå Erreur: {str(e)}"}

    def refine_similarity_with_ai(self, base_ticket, similar_tickets):
        """
        Utilise l'API LM Studio pour affiner la similarit√© des tickets avec analyse IA
        """
        try:
            if not similar_tickets:
                return []

            # Formater les tickets similaires pour le prompt
            tickets_text = "\n\n".join([
                f"Ticket ID: {ticket['ticket_id']}\n"
                f"Probl√®me: {ticket['problem']}\n"
                f"Solution: {ticket['solution']}\n"
                f"Score de similarit√© initial: {ticket['similarity_score']:.2%}"
                for ticket in similar_tickets
            ])

            prompt = f"""
            T√¢che: Analyser la similarit√© s√©mantique entre un ticket de base et des tickets similaires.

            Instructions:
            1. Comparez chaque ticket avec le ticket de base
            2. √âvaluez la similarit√© s√©mantique (0-100%)
            3. Identifiez les aspects techniques correspondants
            4. Fournissez une br√®ve explication de pertinence

            Ticket de base:
            {base_ticket}

            Tickets similaires:
            {tickets_text}

            FORMAT DE R√âPONSE STRICTEMENT JSON:
            [
                {{
                    "ticket_id": "ID du ticket",
                    "similarite_semantique": 75,
                    "aspects_correspondants": ["aspect1", "aspect2"],
                    "explication_pertinence": "Explication concise"
                }}
            ]
            """

            payload = {
                "model": "mistral-nemo-instruct-2407",
                "messages": [{"role": "user", "content": prompt}],
                "temperature": 0.3,
                "max_tokens": 1000
            }

            logger.info("üì§ Envoi de la requ√™te √† l'API IA...")
            response = requests.post(LM_STUDIO_API, json=payload, timeout=180)
            response.raise_for_status()
            
            result = response.json()
            full_response = result["choices"][0]["message"]["content"]
            
            logger.info("üîç R√©ponse compl√®te de l'IA re√ßue")

            # Tentatives multiples de parsing JSON
            try:
                # M√©thode 1 : Parsing JSON direct
                refined_tickets = json.loads(full_response)
                logger.info("‚úÖ Parsing JSON direct r√©ussi")
                return refined_tickets
            except json.JSONDecodeError:
                # M√©thode 2 : Extraction JSON entre ```json et ```
                json_match = re.search(r'```json(.*?)```', full_response, re.DOTALL)
                if json_match:
                    try:
                        refined_tickets = json.loads(json_match.group(1))
                        logger.info("‚úÖ Parsing JSON entre ``` r√©ussi")
                        return refined_tickets
                    except json.JSONDecodeError:
                        logger.warning("‚ùå Parsing JSON entre ``` √©chou√©")

                # M√©thode 3 : Extraction JSON personnalis√©e
                logger.warning("‚ö†Ô∏è Tentative d'extraction JSON personnalis√©e")
                return self._parse_ai_response(full_response)

        except Exception as e:
            logger.error(f"‚ùå Erreur de raffinement par IA: {e}")
            return []
    
    def set_excel_file_path(self, path):
        """
        Met √† jour le chemin du fichier Excel √† traiter
        """
        self.fichier_excel = path
        logger.info(f"üìÇ Mise √† jour du chemin du fichier Excel: {path}")
        return self
    
    def _parse_ai_response(self, ai_response):
        """
        M√©thode de repli pour parser la r√©ponse de l'IA
        """
        try:
            # Ajout de regex plus robuste
            refined_tickets = []
            pattern = re.compile(
                r'"ticket_id"\s*:\s*"?(\S+)"?.*'
                r'"similarite_semantique"\s*:\s*(\d+).*'
                r'"aspects_correspondants"\s*:\s*\[(.*?)\].*'
                r'"explication_pertinence"\s*:\s*"(.*?)"',
                re.DOTALL | re.IGNORECASE
            )

            matches = list(pattern.finditer(ai_response))
            
            for match in matches:
                ticket_id = match.group(1).strip('"')
                similarite = int(match.group(2))
                aspects = [aspect.strip().strip('"') for aspect in match.group(3).split(',') if aspect.strip()]
                explication = match.group(4).strip()

                refined_tickets.append({
                    "ticket_id": ticket_id,
                    "similarite_semantique": similarite,
                    "aspects_correspondants": aspects,
                    "explication_pertinence": explication
                })

            logger.info(f"‚úÖ Extraction manuelle : {len(refined_tickets)} tickets raffin√©s")
            return refined_tickets

        except Exception as e:
            logger.error(f"‚ùå √âchec de l'extraction manuelle : {e}")
            return []

    def traiter_fichier_excel(self):
        """
        Traite le fichier Excel configur√© contenant des tickets √† rechercher.
        """
        # Dans la m√©thode traiter_fichier_excel
        logger.info(f"D√©but du traitement du fichier: {self.fichier_excel}")
        logger.info(f"Configuration actuelle: {self.config}")
        # V√©rifiez si le mod√®le d'embedding est correctement charg√©
        logger.info(f"Mod√®le: {self.model}")
        try:
            if not pd.io.common.is_file_like(self.fichier_excel) and not pd.io.common.is_url(self.fichier_excel):
                if not os.path.exists(self.fichier_excel):
                    logger.error(f"‚ùå Le fichier {self.fichier_excel} n'existe pas.")
                    return []
                    
            df = pd.read_excel(self.fichier_excel)
            resultats = []
            
            logger.info(f"üìë Traitement de {len(df)} tickets depuis le fichier Excel...")
            total_search_time = 0
            
            for idx, row in df.iterrows():
                logger.info(f"üîç Analyse du ticket #{idx+1}...")
                # Combiner toutes les colonnes non-nulles pour cr√©er le texte du ticket
                raw_ticket_text = " ".join(str(value) for value in row.values if pd.notna(value))
                # Rechercher des tickets similaires avec embeddings
                resultat = self.rechercher_tickets_similaires(raw_ticket_text)
                resultats.append(resultat)
                if "temps_recherche" in resultat:
                    total_search_time += resultat["temps_recherche"]
                
            avg_search_time = total_search_time / len(df) if len(df) > 0 else 0
            logger.info(f"üéØ Traitement termin√©. {len(resultats)} tickets analys√©s.")
            logger.info(f"‚è±Ô∏è Temps moyen de recherche par ticket: {avg_search_time:.4f} secondes")
            
            # Calculer des statistiques
            succes = sum(1 for r in resultats if r.get("status") == "success")
            taux_succes = (succes / len(resultats)) * 100 if resultats else 0
            logger.info(f"üìä Taux de succ√®s: {taux_succes:.1f}% ({succes}/{len(resultats)})")
            
            # Sauvegarder les r√©sultats dans un fichier CSV
            self.sauvegarder_resultats_csv(resultats)
            
            return resultats
        except Exception as e:
            logger.error(f"‚ùå Erreur lors du traitement du fichier Excel: {e}")
            return []
    def sauvegarder_resultats_csv(self, resultats):
        """
        Sauvegarde les r√©sultats de recherche dans un fichier CSV
        """
        if not resultats:
            logger.warning("‚ö† Aucun r√©sultat √† sauvegarder.")
            return
           
        resultats_simples = []
        for r in resultats:
            if r.get("status") == "success" and "tickets" in r:
                for ticket in r["tickets"]:
                    resultats_simples.append({
                        "probleme_original": r.get("query", ""),
                        "ticket_similaire_id": ticket["ticket_id"],
                        "probleme_similaire": ticket["problem"],
                        "solution_proposee": ticket["solution"],
                        "score_similarite": ticket["similarity_score"],
                        "temps_recherche": r.get("temps_recherche", 0)
                    })
       
        if resultats_simples:
            df_resultats = pd.DataFrame(resultats_simples)
            output_file = "resultats_recherche_embeddings.csv"
            df_resultats.to_csv(output_file, index=False)
            logger.info(f"‚úÖ R√©sultats sauvegard√©s dans '{output_file}'")
        else:
            logger.warning("‚ö† Aucun r√©sultat de similarit√© √† sauvegarder.")

def main():
    try:
        recherche = RechercheTicketsEmbeddingsOptimized()
        # V√©rifier et g√©n√©rer les embeddings si n√©cessaire
        if recherche.verifier_vecteurs_cache() and recherche.generer_embeddings_manquants():
            print(f"\nüìë Configuration pr√™te pour traiter des fichiers")
        else:
            print("‚ö† V√©rifiez que la base contient des tickets avant d'effectuer des recherches.")
        return recherche
    except Exception as e:
        print(f"‚ùå Erreur principale: {e}")
        import traceback
        print(traceback.format_exc())
        return None
    
if __name__ == "__main__":
    main()